---
title: "assessment1 part3"
author: "Yalan Huang S4664845"
date: "28/08/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#library all package we needed for part3
```{r}
library(tidytext)
library(tidyverse)
library(lubridate)
library(textdata)
library(dplyr)
library(RColorBrewer)
library(wordcloud)
library(wordcloud2)

library(topicmodels)
library(tm)

```

#merging and opening all csv files

```{r}
files<-list.files(pattern="\\.csv$",full.names = TRUE) 
all_data<-map_df(files,  ~read_csv(.x))

```

```{r}
final_data<-all_data%>%distinct()
```

#import twitter data

```{r}
football_final<-final_data%>%
  select(id, in_reply_to_screen_name, created_at, text, favorite_count, retweet_count)
```

```{r}
tweets<-
  football_final%>%
  mutate(
    timestamp=ymd_hms(created_at),
    day_of_week=wday(timestamp),
    id=as_factor(row_number())
  )
```

#Deletes a subkey or entries from the registry.

```{r}
remove_reg <- "&amp;|&lt;|&gt;|\\d+\\w*\\d*|#\\w+|[^\x01-\x7F]|[[:punct:]]|https\\S*"
# &amp = @
# &lt;= <
# &gt; >
```


#removing retweets characters

```{r}

tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) 

```

#unnesting tweets 

```{r}

tidy_tweets<-tidy_tweets%>%
  unnest_tokens(word, text, token = "tweets") 

```

#Remove mentions, urls, emojis, numbers, punctuations, etc

```{r}
tidy_tweets<-tidy_tweets%>%
  filter(
    str_detect(word, "[a-z]")#keep only character-words
  )%>%
  anti_join((stop_words))

```

```{r}
head(tidy_tweets$word)
```

#get sentiment
```{r}
bing<-get_sentiments("bing")
```

```{r}
selected_tweets_tidy<-tidy_tweets%>%select(timestamp, day_of_week, word, id)
```

#sentiment analysis
```{r}
selected_tweets_tidy<-selected_tweets_tidy%>%
  inner_join(bing)%>%
  rename(sentiment_bing=sentiment)

```

```{r}
selected_tweets_tidy%>%
  count(sentiment_bing, sort=TRUE)
```

```{r}
tidy_tweets%>%count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

#topic modeling
#create a document term frequency based on how often words are

```{r}
tweets_dtm<-tidy_tweets%>%
  count(id, word)%>%
  cast_dtm( #converts our data to a special object for R = document term frequency matrix
    document=id,
    term=word,
    value=n,
    weighting=tm::weightTf
  )
```

```{r}
tweets_dtm
```

#to speed up processing let's remove those words that are VERY rare
#Find the sum of words in each Document

```{r}
tweets_dtm_trim<-tm::removeSparseTerms(tweets_dtm, sparse=.99)
```

#Find the sum of words in each Document

```{r}
rowTotals <- apply(tweets_dtm_trim, 1, sum) 
tweets_dtm_trim <- tweets_dtm_trim[rowTotals> 0, ]
```

```{r}
tweets_dtm_trim
```

#LDA for 3 topics

```{r}
ap_lda <- LDA(tweets_dtm_trim, k=3, control = list(seed=1234))
```

```{r}
ap_lda
```

```{r}
ap_topics <- tidy(ap_lda, matrix="beta" )
```

#let's look at them with 10 top words for each topic

```{r}

ap_top_terms <- ap_topics %>%
  group_by(topic)%>%
  slice_max(beta, n=10)%>%
  ungroup()%>%
  arrange(topic, -beta)

```

```{r}

ap_top_terms%>%
  mutate(term=reorder_within(term, beta, topic))%>%
  ggplot(aes(beta, term, fill = factor(topic)))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales= "free")+
  scale_y_reordered()

```

